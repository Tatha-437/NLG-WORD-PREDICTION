# -*- coding: utf-8 -*-
"""nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pHH7Jc8QISwrBpCHNO3ljy-fz2iX86jK
"""

import numpy as np
from __future__ import print_function
from keras.callbacks import LambdaCallback
from keras.utils.data_utils import get_file
import random
from google.colab import drive
import io
np.random.seed(42)
import tensorflow as tf
tf.set_random_seed(42)
from keras.models import Sequential, load_model
from keras.layers import Dense, Activation
from keras.layers import LSTM, Dropout 
from keras.layers import TimeDistributed
from keras.layers.core import Dense, Activation, Dropout, RepeatVector 
from keras.optimizers import RMSprop 
import matplotlib.pyplot as plt 
import pickle 
import sys 
import heapq
from pylab import rcParams

path = get_file('nietzsche.txt',origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
with io.open(path, encoding='utf-8') as f:
    text = f.read().lower()
print('corpus length:', len(text))

print(text)

text = open('XYinput.txt').read().lower()
print('corpus length:', len(text))

chars=sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(chars)) 
indices_char = dict((i, c) for i, c in enumerate(chars))
SEQUENCE_LENGTH = 40
step = 3 
sentences = []
next_chars = [] 
for i in range(0, len(text) - SEQUENCE_LENGTH, step): 
    sentences.append(text[i: i + SEQUENCE_LENGTH]) 
    next_chars.append(text[i + SEQUENCE_LENGTH])

X = np.zeros((len(sentences), SEQUENCE_LENGTH, len(chars)), dtype=np.bool) 
y = np.zeros((len(sentences), len(chars)), dtype=np.bool) 
for i, sentence in enumerate(sentences):
   for t, char in enumerate(sentence):
       X[i, t, char_indices[char]] = 1
   y[i, char_indices[next_chars[i]]] = 1

model = Sequential() 
model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, len(chars)))) 
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

optimizer = RMSprop(lr=0.01) 
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

history = model.fit(X, y, validation_split=0.5, batch_size=128, epochs=10, shuffle=True).history

model.save('keras_model.h5')
pickle.dump(history, open("history.p", "wb"))

model = load_model('keras_model.h5') 
history = pickle.load(open("history.p", "rb"))

plt.plot(history['acc'])
plt.plot(history['val_acc']) 
plt.title('model accuracy')
plt.ylabel('accuracy') 
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper left');
plt.show()

plt.plot(history['loss']) 
plt.plot(history['val_loss']) 
plt.title('model loss') 
plt.ylabel('loss')
plt.xlabel('epoch') 
plt.legend(['train', 'test'], loc='upper left'); 
plt.show()

def prepare_input(text): 
   x = np.zeros((1, SEQUENCE_LENGTH, len(chars))) 
   for t, char in enumerate(text):
      x[0, t, char_indices[char]] = 1.
      return x

def sample(preds, top_n=3):
  preds = np.asarray(preds).astype('float64') 
  preds = np.log(preds)
  exp_preds = np.exp(preds) 
  preds = exp_preds / np.sum(exp_preds) 
  return heapq.nlargest(top_n, range(len(preds)), preds.take)

def predict_completion(text): 
   original_text = text 
   generated = text
   completion = ''
   while True:
     x = prepare_input(text)
     preds = model.predict(x, verbose=0)[0]
     next_index = sample(preds, top_n=1)[0]
     next_char = indices_char[next_index]
     text = text[1:] + next_char 
     completion += next_char
     if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':
         return completion

def predict_completions(text, n=3):
   x = prepare_input(text) 
   preds = model.predict(x, verbose=0)[0]
   next_indices = sample(preds, n)
   return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx innext_indices]